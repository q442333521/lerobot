#!/usr/bin/env python3
"""
SmolVLA ç­–ç•¥è¯„ä¼°è„šæœ¬ (MuJoCo ç¯å¢ƒ)

è¿™ä¸ªè„šæœ¬åœ¨ MuJoCo ä»¿çœŸç¯å¢ƒä¸­è¯„ä¼°è®­ç»ƒå¥½çš„ SmolVLA æ¨¡å‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python 03_evaluate_policy.py --model_path outputs/mujoco_smolvla/checkpoint-20000

åŠŸèƒ½:
    - åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    - åœ¨ MuJoCo ç¯å¢ƒä¸­è¿è¡Œå¤šä¸ª episodes
    - ç»Ÿè®¡æˆåŠŸç‡å’Œæ€§èƒ½æŒ‡æ ‡
    - å¯é€‰ä¿å­˜è§†é¢‘
"""

import argparse
import json
import logging
from pathlib import Path
import time

import gymnasium as gym
import numpy as np
import torch

from lerobot.rl.gym_manipulator import make_robot_env, make_processors, GymManipulatorConfig
from lerobot.envs.configs import HILSerlRobotEnvConfig
from lerobot.rl.gym_manipulator import DatasetConfig
from lerobot.processor import TransitionKey, create_transition

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_policy(model_path: str, device: str):
    """åŠ è½½è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹"""
    from lerobot.policies.smolvla import SmolVLAPolicy

    logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
    policy = SmolVLAPolicy.from_pretrained(model_path)
    policy = policy.to(device)
    policy.eval()

    param_count = sum(p.numel() for p in policy.parameters()) / 1e6
    logger.info(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    logger.info(f"  å‚æ•°é‡: {param_count:.1f}M")
    logger.info(f"  è®¾å¤‡: {device}")

    return policy


def evaluate_policy(
    policy,
    env,
    env_processor,
    action_processor,
    num_episodes: int = 10,
    max_steps: int = 200,
    render: bool = True,
    save_video: bool = False,
    video_dir: Path = None
):
    """è¯„ä¼°ç­–ç•¥æ€§èƒ½"""

    logger.info(f"\nğŸ§ª å¼€å§‹è¯„ä¼° ({num_episodes} episodes)")
    logger.info(f"æ¯ä¸ª episode æœ€å¤š {max_steps} æ­¥")
    logger.info("=" * 70)

    episode_rewards = []
    episode_lengths = []
    episode_successes = []

    for episode_idx in range(num_episodes):
        logger.info(f"\nğŸ“ Episode {episode_idx + 1}/{num_episodes}")

        # Reset environment
        obs, info = env.reset()
        env_processor.reset()
        action_processor.reset()

        # Create initial transition
        transition = create_transition(observation=obs, info=info)
        transition = env_processor(transition)

        episode_reward = 0.0
        episode_step = 0
        done = False

        frames = [] if save_video else None

        while episode_step < max_steps and not done:
            # Get action from policy
            with torch.no_grad():
                # ä»è§‚æµ‹ä¸­é€‰æ‹©åŠ¨ä½œ
                obs_tensor = transition[TransitionKey.OBSERVATION]
                action = policy.select_action(obs_tensor)

            # Process action
            transition[TransitionKey.ACTION] = action
            transition[TransitionKey.OBSERVATION] = (
                env.get_raw_joint_positions() if hasattr(env, "get_raw_joint_positions") else {}
            )

            processed_transition = action_processor(transition)
            processed_action = processed_transition[TransitionKey.ACTION]

            # Execute action
            obs, reward, terminated, truncated, info = env.step(processed_action)
            done = terminated or truncated

            # Record
            episode_reward += reward
            episode_step += 1

            # Create new transition
            new_transition = create_transition(
                observation=obs,
                action=processed_action,
                reward=reward,
                done=terminated,
                truncated=truncated,
                info=info
            )
            transition = env_processor(new_transition)

            # Render
            if render:
                env.render()

            # Save frame for video
            if save_video and frames is not None:
                # è·å–æ¸²æŸ“çš„å›¾åƒ
                # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ gym_hil ç¯å¢ƒ API è°ƒæ•´
                pass

            # Progress
            if episode_step % 10 == 0:
                print(f"\r  Step {episode_step}/{max_steps}, Reward: {episode_reward:.2f}", end="", flush=True)

        print()  # æ¢è¡Œ

        # Record episode stats
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_step)

        # Determine success (éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å®šä¹‰)
        # è¿™é‡Œå‡è®¾ reward > 0 è¡¨ç¤ºæˆåŠŸ
        success = episode_reward > 0
        episode_successes.append(success)

        logger.info(f"  å®Œæˆ: {episode_step} æ­¥, å¥–åŠ±: {episode_reward:.2f}, æˆåŠŸ: {success}")

        # Save video if requested
        if save_video and frames and video_dir:
            video_path = video_dir / f"episode_{episode_idx:03d}.mp4"
            # è¿™é‡Œéœ€è¦å®ç°è§†é¢‘ä¿å­˜é€»è¾‘
            logger.info(f"  è§†é¢‘å·²ä¿å­˜: {video_path}")

    # Print summary
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»")
    logger.info("=" * 70)
    logger.info(f"Episodes: {num_episodes}")
    logger.info(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    logger.info(f"å¹³å‡é•¿åº¦: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f} æ­¥")
    logger.info(f"æˆåŠŸç‡: {np.mean(episode_successes) * 100:.1f}% ({sum(episode_successes)}/{num_episodes})")
    logger.info(f"æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
    logger.info(f"æœ€ä½å¥–åŠ±: {np.min(episode_rewards):.2f}")
    logger.info("=" * 70)

    return {
        "episode_rewards": episode_rewards,
        "episode_lengths": episode_lengths,
        "episode_successes": episode_successes,
        "mean_reward": float(np.mean(episode_rewards)),
        "std_reward": float(np.std(episode_rewards)),
        "success_rate": float(np.mean(episode_successes)),
    }


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼° SmolVLA ç­–ç•¥")

    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„"
    )

    # ç¯å¢ƒé…ç½®
    parser.add_argument(
        "--config_path",
        type=str,
        default="mujoco_so101_config.json",
        help="ç¯å¢ƒé…ç½®æ–‡ä»¶è·¯å¾„"
    )

    # è¯„ä¼°å‚æ•°
    parser.add_argument(
        "--num_episodes",
        type=int,
        default=10,
        help="è¯„ä¼°çš„ episodes æ•°é‡"
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=200,
        help="æ¯ä¸ª episode çš„æœ€å¤§æ­¥æ•°"
    )

    # å¯è§†åŒ–
    parser.add_argument(
        "--render",
        action="store_true",
        help="æ˜¯å¦æ¸²æŸ“ç¯å¢ƒ"
    )
    parser.add_argument(
        "--save_video",
        action="store_true",
        help="æ˜¯å¦ä¿å­˜è§†é¢‘"
    )
    parser.add_argument(
        "--video_dir",
        type=str,
        default="videos",
        help="è§†é¢‘ä¿å­˜ç›®å½•"
    )

    # è®¾å¤‡
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu"],
        help="æ¨ç†è®¾å¤‡"
    )

    # ç»“æœä¿å­˜
    parser.add_argument(
        "--save_results",
        action="store_true",
        help="ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON"
    )

    args = parser.parse_args()

    # æ£€æŸ¥è®¾å¤‡
    if args.device == "cuda" and not torch.cuda.is_available():
        logger.warning("CUDA ä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ° CPU")
        args.device = "cpu"

    # åŠ è½½é…ç½®
    logger.info(f"åŠ è½½ç¯å¢ƒé…ç½®: {args.config_path}")
    with open(args.config_path, 'r') as f:
        config_dict = json.load(f)

    # ä¿®æ”¹é…ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    config_dict["mode"] = None  # ä¸å½•åˆ¶æ•°æ®
    config_dict["device"] = args.device
    config_dict["env"]["processor"]["observation"]["display_cameras"] = args.render

    # æ£€æŸ¥ä¾èµ–
    try:
        import gym_hil
        logger.info("âœ“ gym_hil å·²å®‰è£…")
    except ImportError:
        logger.error("âœ— gym_hil æœªå®‰è£…!")
        logger.error("è¯·è¿è¡Œ: pip install -e '.[hilserl]'")
        return

    # æ„å»ºé…ç½®å¯¹è±¡
    env_config = HILSerlRobotEnvConfig(**config_dict['env'])
    dataset_config = DatasetConfig(**config_dict['dataset'])

    gym_config = GymManipulatorConfig(
        env=env_config,
        dataset=dataset_config,
        mode=config_dict['mode'],
        device=config_dict['device']
    )

    logger.info("\nğŸš€ åˆå§‹åŒ–ç¯å¢ƒ...")

    try:
        # åŠ è½½æ¨¡å‹
        policy = load_policy(args.model_path, args.device)

        # åˆ›å»ºç¯å¢ƒ (è¯„ä¼°æ¨¡å¼ä¸éœ€è¦ teleop)
        env, _ = make_robot_env(gym_config.env)
        logger.info("âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºå¤„ç†å™¨
        env_processor, action_processor = make_processors(
            env, None, gym_config.env, gym_config.device
        )
        logger.info("âœ“ å¤„ç†å™¨åˆ›å»ºæˆåŠŸ")

        # åˆ›å»ºè§†é¢‘ç›®å½•
        video_dir = None
        if args.save_video:
            video_dir = Path(args.video_dir)
            video_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ“ è§†é¢‘å°†ä¿å­˜åˆ°: {video_dir}")

        # ç¡®è®¤å¼€å§‹
        input("\næŒ‰ Enter å¼€å§‹è¯„ä¼°...")

        # è¿è¡Œè¯„ä¼°
        results = evaluate_policy(
            policy=policy,
            env=env,
            env_processor=env_processor,
            action_processor=action_processor,
            num_episodes=args.num_episodes,
            max_steps=args.max_steps,
            render=args.render,
            save_video=args.save_video,
            video_dir=video_dir
        )

        # ä¿å­˜ç»“æœ
        if args.save_results:
            results_file = Path(args.model_path).parent / "eval_results.json"
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2)
            logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

        logger.info("\nâœ… è¯„ä¼°å®Œæˆ!")

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
    except Exception as e:
        logger.error(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        if 'env' in locals():
            env.close()
            logger.info("ç¯å¢ƒå·²å…³é—­")


if __name__ == "__main__":
    main()
